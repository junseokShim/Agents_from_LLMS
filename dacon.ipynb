{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsdhAfzvu2pk/6AuJ8oC/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junseokShim/Agents_from_LLMS/blob/main/dacon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYz57TyGoJdt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 디바이스 설정 (M3 칩에서 MPS 우선 사용)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device.type)\n",
        "\n",
        "# 모델 로드 (경량 LLM, MPS 호환)\n",
        "model_name = \"nlpai-lab/kullm-polyglot-13.6b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Ou_Mcn9NoTKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 객관식 여부 판단 함수\n",
        "def is_multiple_choice(question_text):\n",
        "    lines = question_text.strip().split(\"\\n\")\n",
        "    option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
        "    return option_count >= 2\n",
        "\n",
        "# 질문과 선택지 분리 함수\n",
        "def extract_question_and_choices(full_text):\n",
        "    lines = full_text.strip().split(\"\\n\")\n",
        "    q_lines = []\n",
        "    options = []\n",
        "\n",
        "    for line in lines:\n",
        "        if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
        "            options.append(line.strip())\n",
        "        else:\n",
        "            q_lines.append(line.strip())\n",
        "\n",
        "    question = \" \".join(q_lines)\n",
        "    return question, options\n",
        "\n",
        "# 인터넷 검색을 통한 추가 정보 수집 함수\n",
        "def web_search(query):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        search_url = f\"https://www.google.com/search?q={query}\"\n",
        "        response = requests.get(search_url, headers=headers, timeout=5)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        snippets = soup.select('div.BNeawe.s3v9rd.AP7Wnd')\n",
        "        results = ' '.join(snippet.get_text() for snippet in snippets[:3])\n",
        "        return results\n",
        "    except requests.exceptions.RequestException:\n",
        "        return \"\"\n",
        "\n",
        "# 프롬프트 생성기\n",
        "def make_prompt_auto(text):\n",
        "    additional_info = web_search(text)\n",
        "\n",
        "    if is_multiple_choice(text):\n",
        "        question, options = extract_question_and_choices(text)\n",
        "        prompt = (\n",
        "            \"당신은 금융보안 전문가입니다. 제공된 추가 정보와 질문을 고려하여 가장 적절한 정답 번호만 간단히 제시하세요.\\n\\n\"\n",
        "            f\"추가 정보: {additional_info}\\n\\n\"\n",
        "            f\"질문: {question}\\n\"\n",
        "            \"선택지:\\n\"\n",
        "            f\"{chr(10).join(options)}\\n\\n\"\n",
        "            \"정답 번호:\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"당신은 금융보안 전문가입니다. 제공된 추가 정보를 참고하여 아래 질문에 간결하고 명료한 답변을 제공하세요.\\n\\n\"\n",
        "            f\"추가 정보: {additional_info}\\n\\n\"\n",
        "            f\"질문: {text}\\n\\n\"\n",
        "            \"답변:\"\n",
        "        )\n",
        "    return prompt\n",
        "\n",
        "# 로컬 모델을 통한 답변 생성 함수 (직접 generate 사용)\n",
        "def get_local_model_response(prompt, max_new_tokens=300):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text[len(prompt):].strip()\n",
        "\n",
        "# 예시 활용 코드\n",
        "def generate_answer_from_text(text):\n",
        "    prompt = make_prompt_auto(text)\n",
        "    response = get_local_model_response(prompt)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "SGjU3wx6ox70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfKgFEOWQnUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # 데이터 로드\n",
        "    test = pd.read_csv('./data/test.csv')\n",
        "\n",
        "    # 데이터프레임에 적용\n",
        "    answers = []\n",
        "    for idx, row in tqdm(test.iterrows(), total=len(test)):\n",
        "        answer = generate_answer_from_text(row['Question'])\n",
        "        answers.append(answer)\n",
        "\n",
        "    test['generated_answer'] = answers\n",
        "\n",
        "    # 결과 확인 및 저장\n",
        "    sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
        "    sample_submission['Answer'] = answers\n",
        "    sample_submission.to_csv('./submission_gpu.csv', index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "EzHxHlRCo3yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-vqg8aGopBlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}