{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyModg5E7+IVpyanhZerxSbK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junseokShim/Agents_from_LLMS/blob/main/dacon_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP-yBriHQ9VN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('./data/test.csv')"
      ],
      "metadata": {
        "id": "c60qVs01RG_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 객관식 여부 판단 함수\n",
        "def is_multiple_choice(question_text):\n",
        "    \"\"\"\n",
        "    객관식 여부를 판단: 2개 이상의 숫자 선택지가 줄 단위로 존재할 경우 객관식으로 간주\n",
        "    \"\"\"\n",
        "    lines = question_text.strip().split(\"\\n\")\n",
        "    option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
        "    return option_count >= 2\n",
        "\n",
        "\n",
        "# 질문과 선택지 분리 함수\n",
        "def extract_question_and_choices(full_text):\n",
        "    \"\"\"\n",
        "    전체 질문 문자열에서 질문 본문과 선택지 리스트를 분리\n",
        "    \"\"\"\n",
        "    lines = full_text.strip().split(\"\\n\")\n",
        "    q_lines = []\n",
        "    options = []\n",
        "\n",
        "    for line in lines:\n",
        "        if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
        "            options.append(line.strip())\n",
        "        else:\n",
        "            q_lines.append(line.strip())\n",
        "\n",
        "    question = \" \".join(q_lines)\n",
        "    return question, options"
      ],
      "metadata": {
        "id": "Iftl2i1uRLIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프롬프트 생성기\n",
        "def make_prompt_auto(text):\n",
        "    if is_multiple_choice(text):\n",
        "        question, options = extract_question_and_choices(text)\n",
        "        prompt = (\n",
        "                \"당신은 금융보안 전문가입니다.\\n\"\n",
        "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
        "                f\"질문: {question}\\n\"\n",
        "                \"선택지:\\n\"\n",
        "                f\"{chr(10).join(options)}\\n\\n\"\n",
        "                \"답변:\"\n",
        "                )\n",
        "    else:\n",
        "        prompt = (\n",
        "                \"당신은 금융보안 전문가입니다.\\n\"\n",
        "                \"아래 주관식 질문에 대해 정확하고 간략한 설명을 작성하세요.\\n\\n\"\n",
        "                f\"질문: {text}\\n\\n\"\n",
        "                \"답변:\"\n",
        "                )\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "SX_-xNm5RMy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mistralai/Mistral-7B-v0.1\n",
        "# meta-llama/Meta-Llama-3-8B\n",
        "# beomi/KoAlpaca-Polyglot-5.8B : 0.19\n",
        "#model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "model_name = \"beomi/KoAlpaca-Polyglot-5.8B\"\n",
        "\n",
        "# Tokenizer 및 모델 로드 (4bit)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,       # float16 사용\n",
        "    device_map=\"auto\",               # GPU/CPU 자동 분산\n",
        "    low_cpu_mem_usage=True,          # 로딩 시 CPU 메모리 절약\n",
        "    use_safetensors=True             # safetensors 파일 사용 시\n",
        ")\n",
        "\n",
        "# Inference pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "NCcaIUQ1RQpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 후처리 함수\n",
        "def extract_answer_only(generated_text: str, original_question: str) -> str:\n",
        "    \"\"\"\n",
        "    - \"답변:\" 이후 텍스트만 추출\n",
        "    - 객관식 문제면: 정답 숫자만 추출 (실패 시 전체 텍스트 또는 기본값 반환)\n",
        "    - 주관식 문제면: 전체 텍스트 그대로 반환\n",
        "    - 공백 또는 빈 응답 방지: 최소 \"미응답\" 반환\n",
        "    \"\"\"\n",
        "    # \"답변:\" 기준으로 텍스트 분리\n",
        "    if \"답변:\" in generated_text:\n",
        "        text = generated_text.split(\"답변:\")[-1].strip()\n",
        "    else:\n",
        "        text = generated_text.strip()\n",
        "\n",
        "    # 공백 또는 빈 문자열일 경우 기본값 지정\n",
        "    if not text:\n",
        "        return \"미응답\"\n",
        "\n",
        "    # 객관식 여부 판단\n",
        "    is_mc = is_multiple_choice(original_question)\n",
        "\n",
        "    if is_mc:\n",
        "        # 숫자만 추출\n",
        "        match = re.match(r\"\\D*([1-9][0-9]?)\", text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            # 숫자 추출 실패 시 \"0\" 반환\n",
        "            return \"0\"\n",
        "    else:\n",
        "        return text"
      ],
      "metadata": {
        "id": "cGy6xmA1RZfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "\n",
        "for q in tqdm(test['Question'], desc=\"Inference\"):\n",
        "    prompt = make_prompt_auto(q)\n",
        "    output = pipe(prompt, max_new_tokens=128, temperature=0.2, top_p=0.9)\n",
        "    pred_answer = extract_answer_only(output[0][\"generated_text\"], original_question=q)\n",
        "    preds.append(pred_answer)"
      ],
      "metadata": {
        "id": "6WTzDriYRiAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
        "sample_submission['Answer'] = preds\n",
        "sample_submission.to_csv(f'./baseline_submission_{model_name.split(\"/\")[0]}.csv', index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "M3LRUHYKRjbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFscR4FiRqWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}